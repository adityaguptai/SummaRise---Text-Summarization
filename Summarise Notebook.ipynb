{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summarise 2.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Noezp00YQtb_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# SummaRise"
      ]
    },
    {
      "metadata": {
        "id": "CQtLLjLLYdwN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Text Summarization"
      ]
    },
    {
      "metadata": {
        "id": "sPPVaY3ckXsi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://miro.medium.com/max/1020/0*ZF4ZOlUt8em024BE.png)\n",
        "\n",
        "Automatic text summarization, or just text summarization, is the process of creating a short and coherent version of a longer document. Text summarization is the process of distilling the most important information from a source (or sources) to produce an abridged version for a particular user (or users) and task (or tasks). We (humans) are generally good at this type of task as it involves first understanding the meaning of the source document and then distilling the meaning and capturing salient details in the new description.\n",
        "As such, the goal of automatically creating summaries of text is to have the resulting summaries as good as those written by humans.\n",
        "It is not enough to just generate words and phrases that capture the gist of the source document. The summary should be accurate and should read fluently as a new standalone document.\n",
        "Automatic text summarization is the task of producing a concise and fluent summary while preserving key information content and overall meaning\n",
        "\n",
        "#### Why we need automatic text summarization tools.\n",
        "We cannot possibly create summaries of all of the text manually; there is a great need for automatic methods.\n",
        "\n",
        "*   Summaries reduce reading time.\n",
        "*   When researching documents, summaries make the selection process easier.\n",
        "*   Automatic summarization improves the effectiveness of indexing.\n",
        "*   Automatic summarization algorithms are less biased than human summarizers.\n",
        "*   Personalized summaries are useful in question-answering systems as they provide personalized information.\n",
        "*   Using automatic or semi-automatic summarization systems enables commercial abstract services to increase the number of texts they are able to process.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####  A useful list of every-day examples of text summarization.\n",
        "\n",
        "\n",
        "*   headlines (from around the world)\n",
        "*   biography (resumes, obituaries)\n",
        "*   synopses (soap opera listings\n",
        "*   histories (chronologies of salient events)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "J-oksyvue08K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Types\n",
        "There are mainly two ways to make the summary. Extractive and Abstractive.\n",
        "\n",
        "### Extractive\n",
        "Select relevant phrases of the input document and concatenate them to form a summary (like \"copy-and-paste\").\n",
        "\n",
        "*   Pros: They are quite robust since they use existing natural-language phrases that are taken straight from the input.\n",
        "*   Cons: But they lack in flexibility since they cannot use novel words or connectors. They also cannot paraphrase like people sometimes do.\n",
        "\n",
        "\n",
        ">Original Text: Alice and Bob took the train to visit the zoo. They saw a baby giraffe, a lion, and a flock of colorful tropical birds. \n",
        "\n",
        "> Extractive Summary: Alice and Bob visit the zoo. saw a flock of birds\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Now I show the some categories of extractive summarization.\n",
        "\n",
        "---\n",
        "\n",
        "#### Graph Base\n",
        "\n",
        "The graph base model makes the graph from the document, then summarize it by considering the relation between the nodes (text-unit). TextRank is the typical graph based method.\n",
        "\n",
        "TextRank\n",
        "*   https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "#### Feature Base\n",
        "The feature base model extracts the features of sentence, then evaluate its importance. Here is the representative research.\n",
        "\n",
        "Sentence Extraction Based Single Document Summarization\n",
        "\n",
        "\n",
        "*   http://oldwww.iiit.ac.in/cgi-bin/techreports/display_detail.cgi?id=IIIT/TR/2008/97\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "#### Topic Base\n",
        "The topic base model calculates the topic of the document and evaluate each sentences by what kinds of topics are included (the \"main\" topic is highly evaluated when scoring the sentence).\n",
        "\n",
        "Latent Semantic Analysis (LSA) is usually used to detect the topic. It's based on SVD (Singular Value Decomposition).\n",
        "The following paper is good starting point to overview the LSA(Topic) base summarization\n",
        "\n",
        "\n",
        "Text summarization using Latent Semantic Analysis\n",
        "* https://www.researchgate.net/publication/220195824_Text_summarization_using_Latent_Semantic_Analysis\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "#### Grammer Base\n",
        "The grammer base model parses the text and constructs a grammatical structure, then select/reorder substructures.\n",
        "\n",
        "Title Generation with Quasi-Synchronous Grammar\n",
        "\n",
        "\n",
        "*   https://www.aclweb.org/anthology/D/D10/D10-1050.pdf\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "TEXYr-PnPLMi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###  Abstrative\n",
        "\n",
        "Generate a summary that keeps original intent. It's just like humans do.\n",
        "\n",
        "*   Pros: They can use words that were not in the original input. It enables to make more fluent and natural summaries.\n",
        "*   Cons: But it is also a much harder problem as you now require the model to generate coherent phrases and connectors.\n",
        "\n",
        "For Example:\n",
        ">Original Text: Alice and Bob took the train to visit the zoo. They saw a baby giraffe, a lion, and a flock of colorful tropical birds. \n",
        "\n",
        ">Abstractive summary: Alice and Bob visited the zoo and saw animals and birds\n",
        "\n",
        "Now I show the some categories of abstractive summarization.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### Encoder-Decoder Model\n",
        "The encoder-decoder model is composed of encoder and decoder like its name. The encoder converts an input document to a latent representation (vector), and the decoder generates a summary by using it.\n",
        "\n",
        "![alt text](https://github.com/icoxfog417/awesome-text-summarization/raw/master/images/encoder_decoder.png)\n",
        "But the encoder-decoder model is not the silver bullet. There are many remaining issues are there.\n",
        "\n",
        "\n",
        "*   How to set the focus on the important sentence, keyword.\n",
        "*   How to handle the novel/rare (but important) word in source document.\n",
        "*   How to handle the long document.\n",
        "*   Want to make more human-readable summary.\n",
        "\n",
        "#### A Neural Attention Model for Sentence Summarization\n",
        "\n",
        "What is solves?\n",
        "* How to set the focus on the important sentence, keyword.?\n",
        "use Attention\n",
        "\n",
        "* How to handle the novel/rare (but important) word in source document.\n",
        "add n-gram match term to the loss function\n",
        "\n",
        "Other features\n",
        "* use 1D convolution to capture the local context\n",
        "* use beam-search to generate summary\n",
        "\n",
        "#### Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond\n",
        "\n",
        "* How to set the focus on the important sentence, keyword.\n",
        "use enhanced feature such as POS, Named Entity tag, TF, IDF \n",
        "* How to handle the novel/rare (but important) word in source document.\n",
        "switch the decoder(generate word) and pointer(copy from original text)\n",
        "* How to handle the long document.\n",
        "use sentence level attention (sec 2.4)\n",
        "* Want to use large vocabulary.\n",
        "use subset of vocabulary on the training (sec 2.1, please refer On Using Very Large Target \n",
        "* Vocabulary for Neural Machine Translation)\n",
        "\n",
        "\n",
        "## Combination Approach\n",
        "\n",
        "#### Pointer-Generator Network\n",
        "* Get To The Point: Summarization with Pointer-Generator Networks\n",
        "* How to set the focus on the important sentence, keyword.\n",
        "use Attention \n",
        "* How to handle the novel/rare (but important) word in source document.\n",
        "switch the decoder(generator) and pointer network (by p_gen probability).\n",
        "combine the distribution of vocabulary and attention with p_gen and (1 - p_gen) weight (please refer the following picture).\n",
        "\n",
        "![alt text](https://github.com/icoxfog417/awesome-text-summarization/blob/master/images/get_to_the_point.png?raw=true)"
      ]
    },
    {
      "metadata": {
        "id": "d5Ohci48RpIv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "\n",
        "1.   https://machinelearningmastery.com/gentle-introduction-text-summarization/\n",
        "2.   https://github.com/icoxfog417/awesome-text-summarization\n",
        "\n"
      ]
    }
  ]
}